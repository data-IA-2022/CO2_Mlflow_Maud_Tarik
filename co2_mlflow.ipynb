{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow 5 minute Tracking Quickstart\n",
    "\n",
    "This notebook demonstrates using a local MLflow Tracking Server to log, register, and then load a model as a generic Python Function (pyfunc) to perform inference on a Pandas dfFrame.\n",
    "\n",
    "Throughout this notebook, we'll be using the MLflow fluent API to perform all interactions with the MLflow Tracking Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mlflow in /home/tarik/.local/lib/python3.10/site-packages (2.9.1)\n",
      "Requirement already satisfied: click<9,>=7.0 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (8.1.3)\n",
      "Requirement already satisfied: cloudpickle<4 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (3.0.0)\n",
      "Requirement already satisfied: databricks-cli<1,>=0.8.7 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (0.18.0)\n",
      "Requirement already satisfied: entrypoints<1 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (0.4)\n",
      "Requirement already satisfied: gitpython<4,>=2.1.0 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (3.1.40)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/lib/python3/dist-packages (from mlflow) (5.4.1)\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (4.23.0)\n",
      "Requirement already satisfied: pytz<2024 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (2023.3.post1)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (2.31.0)\n",
      "Requirement already satisfied: packaging<24 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (23.1)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /usr/lib/python3/dist-packages (from mlflow) (4.6.4)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (0.4.4)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (1.13.0)\n",
      "Requirement already satisfied: docker<7,>=4.0.0 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (6.1.2)\n",
      "Requirement already satisfied: Flask<4 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (2.3.2)\n",
      "Requirement already satisfied: numpy<2 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (1.26.0)\n",
      "Requirement already satisfied: scipy<2 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (1.11.2)\n",
      "Requirement already satisfied: pandas<3 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (2.1.0)\n",
      "Requirement already satisfied: querystring-parser<2 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (1.2.4)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (2.0.15)\n",
      "Requirement already satisfied: scikit-learn<2 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (1.3.0)\n",
      "Requirement already satisfied: pyarrow<15,>=4.0.0 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (14.0.1)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (3.4.3)\n",
      "Requirement already satisfied: matplotlib<4 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (3.7.1)\n",
      "Requirement already satisfied: gunicorn<22 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (21.2.0)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /home/tarik/.local/lib/python3.10/site-packages (from mlflow) (3.1.2)\n",
      "Requirement already satisfied: Mako in /home/tarik/.local/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/tarik/.local/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (4.8.0)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in /home/tarik/.local/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (2.7.0)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.2.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /home/tarik/.local/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (0.9.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.7 in /home/tarik/.local/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.26.16)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /home/tarik/.local/lib/python3.10/site-packages (from docker<7,>=4.0.0->mlflow) (1.5.2)\n",
      "Requirement already satisfied: Werkzeug>=2.3.3 in /home/tarik/.local/lib/python3.10/site-packages (from Flask<4->mlflow) (2.3.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /home/tarik/.local/lib/python3.10/site-packages (from Flask<4->mlflow) (2.1.2)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /home/tarik/.local/lib/python3.10/site-packages (from Flask<4->mlflow) (1.6.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/tarik/.local/lib/python3.10/site-packages (from gitpython<4,>=2.1.0->mlflow) (4.0.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tarik/.local/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/tarik/.local/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/tarik/.local/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/tarik/.local/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/tarik/.local/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/tarik/.local/lib/python3.10/site-packages (from matplotlib<4->mlflow) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib<4->mlflow) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/tarik/.local/lib/python3.10/site-packages (from matplotlib<4->mlflow) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/tarik/.local/lib/python3.10/site-packages (from pandas<3->mlflow) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tarik/.local/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tarik/.local/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tarik/.local/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (2023.5.7)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/tarik/.local/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/tarik/.local/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (3.2.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/tarik/.local/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (2.0.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/tarik/.local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow) (5.0.1)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/tarik/.local/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/tarik/.local/lib/python3.10/site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/tarik/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/tarik/.local/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/tarik/.local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/tarik/.local/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/tarik/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/tarik/.local/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/tarik/.local/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/tarik/.local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mlflow\n",
      "Version: 2.9.1\n",
      "Summary: MLflow: A Platform for ML Development and Productionization\n",
      "Home-page: https://mlflow.org/\n",
      "Author: Databricks\n",
      "Author-email: \n",
      "License: Apache License 2.0\n",
      "Location: /home/tarik/.local/lib/python3.10/site-packages\n",
      "Requires: alembic, click, cloudpickle, databricks-cli, docker, entrypoints, Flask, gitpython, gunicorn, importlib-metadata, Jinja2, markdown, matplotlib, numpy, packaging, pandas, protobuf, pyarrow, pytz, pyyaml, querystring-parser, requests, scikit-learn, scipy, sqlalchemy, sqlparse\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: seaborn\n",
      "Version: 0.12.2\n",
      "Summary: Statistical data visualization\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Michael Waskom <mwaskom@gmail.com>\n",
      "License: \n",
      "Location: /home/tarik/.local/lib/python3.10/site-packages\n",
      "Requires: matplotlib, numpy, pandas\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline  # Import Pipeline class\n",
    "import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('data/2020b_Building_Energy_Benchmarking.csv', sep=',')\n",
    "\n",
    "# df.dropna(axis=0, inplace=True)\n",
    "df.columns = df.columns.str.lower()\n",
    "df.columns = df.columns.str.replace(\"(\", \"\")\n",
    "df.columns = df.columns.str.replace(\")\", \"\")\n",
    "df.columns\n",
    "\n",
    "\n",
    "if 'yearsenergystarcertified' in df.columns and 'outlier' in df.columns:\n",
    "    df.drop(['yearsenergystarcertified', 'outlier'], axis=1, inplace=True)\n",
    "\n",
    "if 'compliancestatus' in df.columns:\n",
    "    # Filter the dfFrame to keep only df with Compliant in ComplianceStatus\n",
    "    df = df[df[\"compliancestatus\"] == 'Compliant']\n",
    "# Drop the column after check only compliance in compliancesstatus\n",
    "df.drop(['compliancestatus'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Filter the dfFrame to keep only rows where siteenergyusekbtu is not null\n",
    "df = df[df[\"siteenergyusekbtu\"].notnull()]\n",
    "# fill Nan Null with np.nan\n",
    "df = df.fillna(np.nan)\n",
    "# # Replace \"NULL\" with np.nan in your df\n",
    "# df = df.replace(\"NULL\", np.nan).replace(\"NA\", np.nan)\n",
    "# Replace \"NULL\" with np.nan in your df\n",
    "df = df.replace(\"NA\", np.nan)\n",
    "\n",
    "# Add column is elec etc\n",
    "# Create new columns with 1 or 0 based on conditions\n",
    "df['is_using_steamusekWh'] = np.where(df['steamusekbtu'] > 0, 1, 0)\n",
    "df['is_using_electricitykWh'] = np.where(df['electricitykbtu'] > 0, 1, 0)\n",
    "df['is_using_naturalgaskWh'] = np.where(df['naturalgaskbtu'] > 0, 1, 0)\n",
    "\n",
    "# filter column\n",
    "selected_columns = [\"siteenergyusekbtu\", 'totalghgemissions','yearbuilt','is_using_electricitykWh', 'is_using_naturalgaskWh', 'is_using_steamusekWh', 'largestpropertyusetypegfa', 'numberofbuildings', 'numberoffloors', 'propertygfabuildings','buildingtype', 'primarypropertytype']\n",
    "\n",
    "# Filter the DataFrame to select only the desired columns\n",
    "df = df[selected_columns]\n",
    "\n",
    "# drop nan \n",
    "df.dropna(axis=0, inplace=True)\n",
    "\n",
    "coerced_columns = {\n",
    "    'yearbuilt': float,\n",
    "    'numberoffloors': float,\n",
    "    'propertygfabuildings': float,\n",
    "    'numberofbuildings': float,\n",
    "    'is_using_electricitykWh': int,\n",
    "    'is_using_naturalgaskWh': int,\n",
    "    'is_using_steamusekWh': int,\n",
    "}\n",
    "\n",
    "df = df.astype(coerced_columns)\n",
    "\n",
    "\n",
    "# save result as csv\n",
    "df.to_csv(\"data/dataset_2020b.csv\", sep=\",\", index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3196, 12)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "siteenergyusekbtu            0\n",
       "totalghgemissions            0\n",
       "yearbuilt                    0\n",
       "is_using_electricitykWh      0\n",
       "is_using_naturalgaskWh       0\n",
       "is_using_steamusekWh         0\n",
       "largestpropertyusetypegfa    0\n",
       "numberofbuildings            0\n",
       "numberoffloors               0\n",
       "propertygfabuildings         0\n",
       "buildingtype                 0\n",
       "primarypropertytype          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siteenergyusekbtu            float64\n",
      "totalghgemissions            float64\n",
      "yearbuilt                    float64\n",
      "is_using_electricitykWh        int64\n",
      "is_using_naturalgaskWh         int64\n",
      "is_using_steamusekWh           int64\n",
      "largestpropertyusetypegfa    float64\n",
      "numberofbuildings            float64\n",
      "numberoffloors               float64\n",
      "propertygfabuildings         float64\n",
      "buildingtype                  object\n",
      "primarypropertytype           object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "column_types = df.dtypes\n",
    "print(column_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siteenergyusekbtu            float64\n",
      "totalghgemissions            float64\n",
      "yearbuilt                    float64\n",
      "is_using_electricitykWh        int64\n",
      "is_using_naturalgaskWh         int64\n",
      "is_using_steamusekWh           int64\n",
      "largestpropertyusetypegfa    float64\n",
      "numberofbuildings            float64\n",
      "numberoffloors               float64\n",
      "propertygfabuildings         float64\n",
      "buildingtype                  object\n",
      "primarypropertytype           object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset complet for dtypes\n",
    "df_2016 = pd.read_csv('data/dataset_2016.csv', sep=',')\n",
    "column_types = df.dtypes\n",
    "print(column_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siteenergyusekbtu            float64\n",
      "totalghgemissions            float64\n",
      "yearbuilt                    float64\n",
      "is_using_electricitykWh        int64\n",
      "is_using_naturalgaskWh         int64\n",
      "is_using_steamusekWh           int64\n",
      "largestpropertyusetypegfa    float64\n",
      "numberofbuildings            float64\n",
      "numberoffloors               float64\n",
      "propertygfabuildings         float64\n",
      "buildingtype                  object\n",
      "primarypropertytype           object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset complet for dtypes\n",
    "df_2020 = pd.read_csv('data/dataset_2020.csv', sep=',')\n",
    "column_types = df.dtypes\n",
    "print(column_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     yearbuilt        \n",
      "          self   other\n",
      "3156    2015.0  2020.0\n",
      "3157    2015.0  2020.0\n",
      "3158    2015.0  2020.0\n",
      "3161    2015.0  2020.0\n",
      "3166    2015.0  2020.0\n",
      "3167    2015.0  2020.0\n",
      "3168    2015.0  2020.0\n",
      "3169    2015.0  2020.0\n",
      "3170    2015.0  2020.0\n",
      "3171    2015.0  2020.0\n"
     ]
    }
   ],
   "source": [
    "# Assuming df_2016 and df_2020 are your two DataFrames\n",
    "comparison_result = df_2016.compare(df_2020)\n",
    "\n",
    "\n",
    "# Display the rows with differences\n",
    "print(comparison_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our saved model as a Python Function\n",
    "\n",
    "Although we can load our model back as a native scikit-learn format with `mlflow.sklearn.load_model()`, below we are loading the model as a generic Python Function, which is how this model would be loaded for online model serving. We can still use the `pyfunc` representation for batch use cases, though, as is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the MLflow Tracking URI \n",
    "In this step, we're going to use the local MLflow tracking server that we started. \n",
    "\n",
    "If you chose to define a different port when starting the server, apply that port to the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8090\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data and train a simple model\n",
    "\n",
    "For our quickstart, we're going to be using the familiar iris dataset that is included in scikit-learn. Following the split of the data, we're going to train a simple logistic regression classifier on the training data and calculate some error metrics on our holdout test data. \n",
    "\n",
    "Note that the only MLflow-related activities in this portion are around the fact that we're using a `param` dictionary to supply our model's hyperparameters; this is to make logging these settings easier when we're ready to log our model and its associated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['siteenergyusekbtu', 'totalghgemissions', 'yearbuilt',\n",
      "       'is_using_electricitykwh', 'is_using_naturalgaskwh',\n",
      "       'is_using_steamusekwh', 'largestpropertyusetypegfa',\n",
      "       'numberofbuildings', 'numberoffloors', 'propertygfabuildings',\n",
      "       'buildingtype', 'primarypropertytype'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('data/dataset_sample.csv', sep=',')\n",
    "df.dropna(axis=0, inplace=True)\n",
    "df.columns = df.columns.str.lower()\n",
    "df.columns\n",
    "print(df.columns)\n",
    "# Separate your target variables from features\n",
    "X = df.drop([\"siteenergyusekbtu\", 'totalghgemissions'], axis=1)\n",
    "Y = df[[\"siteenergyusekbtu\", 'totalghgemissions']]\n",
    "\n",
    "# Define column transformers for numeric and categorical features\n",
    "numeric_features = ['yearbuilt','is_using_electricitykwh', 'is_using_naturalgaskwh', 'is_using_steamusekwh', 'largestpropertyusetypegfa', 'numberofbuildings', 'numberoffloors', 'propertygfabuildings']\n",
    "categorical_features = ['buildingtype', 'primarypropertytype']\n",
    "\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Combine preprocessing and modeling into a single pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', MultiOutputRegressor(GradientBoostingRegressor()))\n",
    "])\n",
    "\n",
    "# Split your dataset into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define your model hyperparameters\n",
    "model_params = {\n",
    "    'model__estimator__loss': 'huber',\n",
    "    'model__estimator__n_estimators': 500,\n",
    "    'model__estimator__max_depth': 5,\n",
    "    'model__estimator__learning_rate': 0.01\n",
    "}\n",
    "\n",
    "# Train your model\n",
    "pipeline.set_params(**model_params)\n",
    "pipeline.fit(X_train, Y_train)  # Fit the model\n",
    "\n",
    "# Predict on the test set\n",
    "Y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "r2_score_test = r2_score(Y_test, Y_pred)\n",
    "mae_test_score = mean_absolute_error(Y_test, Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siteenergyusekbtu            float64\n",
      "totalghgemissions            float64\n",
      "yearbuilt                    float64\n",
      "is_using_electricitykwh        int64\n",
      "is_using_naturalgaskwh         int64\n",
      "is_using_steamusekwh           int64\n",
      "largestpropertyusetypegfa    float64\n",
      "numberofbuildings            float64\n",
      "numberoffloors               float64\n",
      "propertygfabuildings         float64\n",
      "buildingtype                  object\n",
      "primarypropertytype           object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "column_types = df.dtypes\n",
    "print(column_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an MLflow Experiment\n",
    "\n",
    "In order to group any distinct runs of a particular project or idea together, we can define an Experiment that will group each iteration (runs) together. \n",
    "Defining a unique name that is relevant to what we're working on helps with organization and reduces the amount of work (searching) to find our runs later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/14', creation_time=1702395467376, experiment_id='14', last_update_time=1702395467376, lifecycle_stage='active', name='Seatle_co2_pred_maud_tarik_2', tags={}>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log the experiment in MLflow\n",
    "mlflow.set_experiment(\"Seatle_co2_pred_maud_tarik_2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log the model, hyperparameters, and loss metrics to MLflow.\n",
    "\n",
    "In order to record our model and the hyperparameters that were used when fitting the model, as well as the metrics associated with validating the fit model upon holdout data, we initiate a run context, as shown below. Within the scope of that context, any fluent API that we call (such as `mlflow.log_params()` or `mlflow.sklearn.log_model()`) will be associated and logged together to the same run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tarik/.local/lib/python3.10/site-packages/mlflow/models/signature.py:212: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  inputs = _infer_schema(model_input) if model_input is not None else None\n",
      "Registered model 'data/model_tracking' already exists. Creating a new version of this model...\n",
      "2023/12/13 09:09:31 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: data/model_tracking, version 4\n",
      "Created version '4' of model 'data/model_tracking'.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    # Log model hyperparameters\n",
    "    mlflow.log_params(model_params)\n",
    "\n",
    "    # Log evaluation metrics\n",
    "    mlflow.log_metric(\"R2_score_test\", r2_score_test)\n",
    "    mlflow.log_metric(\"MAE_test_score\", mae_test_score)\n",
    "\n",
    "    # Set tags for additional information\n",
    "    mlflow.set_tag(\"Training Info\", \"GradientBoostingRegressor for your use case\")\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, Y_pred)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=pipeline,\n",
    "        artifact_path=\"data/best_model_GradientBoostingRegressor.pkl\",\n",
    "        signature=signature,\n",
    "        input_example=X_train,\n",
    "        registered_model_name=\"data/model_tracking\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tarik/.local/lib/python3.10/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator RobustScaler from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/tarik/.local/lib/python3.10/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/tarik/.local/lib/python3.10/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator OneHotEncoder from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/tarik/.local/lib/python3.10/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator FunctionTransformer from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/tarik/.local/lib/python3.10/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator ColumnTransformer from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/tarik/.local/lib/python3.10/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator GradientBoostingRegressor from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/tarik/.local/lib/python3.10/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator DummyRegressor from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/tarik/.local/lib/python3.10/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/tarik/.local/lib/python3.10/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator MultiOutputRegressor from version 1.3.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the model and make predictions\n",
    "loaded_model = joblib.load(\"data/best_model_GradientBoostingRegressor.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use our model to predict the iris class type on a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['siteenergyusekbtu', 'totalghgemissions', 'yearbuilt',\n",
      "       'is_using_electricitykWh', 'is_using_naturalgaskWh',\n",
      "       'is_using_steamusekWh', 'largestpropertyusetypegfa',\n",
      "       'numberofbuildings', 'numberoffloors', 'propertygfabuildings',\n",
      "       'buildingtype', 'primarypropertytype'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('data/dataset_2020.csv', sep=',')\n",
    "df.dropna(axis=0, inplace=True)\n",
    "df.columns = df.columns.str.lower()\n",
    "# Define the mapping of old column names to new column names\n",
    "column_name_mapping = {\n",
    "    'is_using_electricitykwh': 'is_using_electricitykWh',\n",
    "    'is_using_naturalgaskwh': 'is_using_naturalgaskWh',\n",
    "    'is_using_steamusekwh': 'is_using_steamusekWh'\n",
    "}\n",
    "\n",
    "# Rename the columns in the DataFrame\n",
    "df.rename(columns=column_name_mapping, inplace=True)\n",
    "\n",
    "print(df.columns)\n",
    "# Separate your target variables from features\n",
    "X_2020 = df.drop([\"siteenergyusekbtu\", 'totalghgemissions'], axis=1)\n",
    "Y_2020 = df[[\"siteenergyusekbtu\", 'totalghgemissions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['yearbuilt', 'is_using_electricitykWh', 'is_using_naturalgaskWh',\n",
       "       'is_using_steamusekWh', 'largestpropertyusetypegfa',\n",
       "       'numberofbuildings', 'numberoffloors', 'propertygfabuildings',\n",
       "       'buildingtype', 'primarypropertytype'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2020.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on test data: 0.8801043109858808\n",
      "MAE on test data: 1622054.09953706\n",
      "Predictions: [[6.46835656e+06 2.95572270e+02]\n",
      " [6.79266062e+06 2.19370201e+02]\n",
      " [7.24977561e+07 1.98145561e+03]\n",
      " ...\n",
      " [3.46600136e+06 1.18387389e+02]\n",
      " [3.22734684e+06 1.46312352e+02]\n",
      " [2.44527715e+06 5.83881132e+01]]\n"
     ]
    }
   ],
   "source": [
    "predictions = loaded_model.predict(X_2020)\n",
    "\n",
    "# Display the results\n",
    "print(\"R2 score on test data:\", r2_score_test)\n",
    "print(\"MAE on test data:\", mae_test_score)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not all strings exist in the DataFrame columns.\n"
     ]
    }
   ],
   "source": [
    "# List of strings to check\n",
    "strings_to_check = ['is_using_steamusekwh', 'is_using_electricitykwh', 'is_using_naturalgaskwh']\n",
    "\n",
    "# Check if all strings in the list exist in the DataFrame columns\n",
    "if all(col in X_2020.columns for col in strings_to_check):\n",
    "    print(\"All strings exist in the DataFrame columns.\")\n",
    "else:\n",
    "    print(\"Not all strings exist in the DataFrame columns.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8_mlflow_venv",
   "language": "python",
   "name": "python3.8_mlflow_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
